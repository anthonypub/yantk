{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our training data, the XOR function. Note the first column, of all ones, will be to compute biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "nonlin = 'sigmoid'\n",
    "X = np.array([(1, -1, -1),(1, -1, 1),(1, 1, -1),(1, 1, 1)])\n",
    "Y = np.array([(1, 0), (0, 1),(0, 1),(1, 0)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are our starting weight matrices. WHI contains the weights projecting from the input layer to the hidden layer, and WOH contains the weights projecting from the hidden layer to the output layer. Note that WHI contains a first \"phantom\" row of all zeros; this is a little hack to for the bias computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHI: \n",
      "[[ 0.         0.         0.       ]\n",
      " [-0.0130107  0.001805   0.0170633]\n",
      " [ 0.0420667  0.0069242  0.0421364]\n",
      " [-0.0082391 -0.0285141  0.0302499]]\n",
      "WOH: \n",
      "[[ 0.0338  0.0046 -0.0072 -0.0052]\n",
      " [ 0.0264  0.0123  0.0093 -0.0101]]\n"
     ]
    }
   ],
   "source": [
    "WHI = np.array([(0, 0, 0),(-0.0130107, 0.0018050, 0.0170633), (0.0420667, 0.0069242, 0.0421364),(-0.0082391, -0.0285141, 0.0302499)])\n",
    "WOH = np.array([(0.0338, 0.0046, -0.0072, -0.0052),(0.0264, 0.0123, 0.0093, -0.0101)])\n",
    "print(\"WHI: \")\n",
    "print(WHI)\n",
    "print(\"WOH: \")\n",
    "print(WOH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start on the forward pass. In each case we will multiply the projection matrix by the input, with training instances in the columns, then applying the nonlinearity. We handle the bias by creating one extra hidden unit and resetting it to \"1\" after the activations have been computed. This requires extra computation, but in a realistically sized net it would be a tiny waste. First we'll define the nonlinearity. For now we are using the sigmoid function, but we will allow the user to specify the nonlinearity so we can use other choices later.\n",
    "\n",
    "To allow for arbitrary numbers of layers we'll pass the weight vectors in as a list, and return a list of activations (which we'll need later for backprop)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the sigmoid to produce the first hidden output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ array([[ 1.        ,  1.        ,  1.        ,  1.        ],\n",
      "       [ 0.49203092,  0.5005619 ,  0.49293322,  0.5014644 ],\n",
      "       [ 0.49825153,  0.51931012,  0.50171362,  0.52276607],\n",
      "       [ 0.4975063 ,  0.51262854,  0.48325549,  0.49837418]])\n",
      " array([[ 0.50747167,  0.50742393,  0.507485  ,  0.50743727],\n",
      "       [ 0.50801454,  0.50805154,  0.50806134,  0.50809833]])]\n",
      "Error: 2.00048188622\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def scalar_rectifier(x):\n",
    "    if x > 0:\n",
    "        return x;\n",
    "    return x * .01\n",
    "\n",
    "def rectifier(x):\n",
    "    return np.vectorize(scalar_rectifier)(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "def forward(x, WEIGHTS, nl):\n",
    "    #note that we transpose x to put the training examples in columns here\n",
    "    inAct = np.transpose(x);\n",
    "    activations=[]\n",
    "    for i in range(0, len(WEIGHTS)):\n",
    "        W = WEIGHTS[i]\n",
    "        NET = np.dot(W, inAct)\n",
    "        if nl == 'sigmoid':\n",
    "            act = sigmoid(NET);\n",
    "        elif nl == 'tanh':\n",
    "            act = np.tanh(NET);\n",
    "        elif nl == 'relu':\n",
    "            act = rectifier(NET);\n",
    "        else:\n",
    "            raise Exception('unsupported nonlinearity')\n",
    "        if i != len(WEIGHTS) - 1:\n",
    "            act[0,:] = 1\n",
    "        activations = activations + [act]\n",
    "        inAct = act\n",
    "    return np.array(activations)\n",
    "\n",
    "def compute_error(A, Y):\n",
    "    last = len(A) - 1\n",
    "    err = Y - A[last]\n",
    "    sqerr = err * err;\n",
    "    return np.sum(sqerr)\n",
    "    \n",
    "\n",
    "WEIGHTS = [WHI, WOH]\n",
    "activations = forward(X, WEIGHTS, 'sigmoid')\n",
    "print(activations)\n",
    "err = compute_error(activations, np.transpose(Y))\n",
    "print(\"Error: \" + str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "That was the easy part. Now let's do the hard part (backprop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update 0\n",
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [ -9.99574814e-05  -5.52225107e-07  -1.26935418e-07]\n",
      " [ -1.50190730e-05   5.35142840e-06   6.61731356e-07]\n",
      " [  9.08288830e-05   4.13802070e-07  -1.62174657e-07]]\n",
      "update 1\n",
      "[[-0.02235842 -0.01110612 -0.01141579 -0.01113385]\n",
      " [-0.02416303 -0.01200326 -0.01233386 -0.01203035]]\n"
     ]
    }
   ],
   "source": [
    "def sigderiv(x):\n",
    "    return (x * (1-x))\n",
    "def tanhderiv(x):\n",
    "    return(1 - (x*x))\n",
    "def recderiv_scalar(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    return 0.01\n",
    "\n",
    "def recderiv(x):\n",
    "    return np.vectorize(recderiv_scalar)(x)\n",
    "    \n",
    "    \n",
    "def nlderiv(x, nl):\n",
    "    if nl == 'sigmoid':\n",
    "        return sigderiv(x)\n",
    "    elif nl == 'tanh':\n",
    "        return tanhderiv(x)\n",
    "    elif nl == 'relu':\n",
    "        return recderiv(x)\n",
    "    else:\n",
    "        raise Exception('unsupported nonlin')\n",
    "#A = activatitions from all layers\n",
    "#Y = targets\n",
    "#nl = nonlinearity type\n",
    "#returns a list of update weights\n",
    "def backprop(X, A, Y, W, nl, rate):\n",
    "    #First compute the gradients\n",
    "    #This is the derivative of the nonlinearity function.\n",
    "    updates = []\n",
    "    nlgrad = np.ones((1))\n",
    "    out_idx = len(A) - 1\n",
    "    gradients = []\n",
    "    nlgrad = nlderiv(A[out_idx], nl)\n",
    "    delta_k = nlgrad * (np.transpose(Y) - A[out_idx])\n",
    "    gradients = [delta_k]\n",
    "    delta_prev = delta_k\n",
    "    for i in range(out_idx - 1, -1, -1):\n",
    "        #print(\"iter \" + str(i))\n",
    "        #Compute the weighted gradient from prev layer\n",
    "        wg = np.transpose(W[i + 1]).dot(delta_prev)\n",
    "        deriv = nlderiv(A[i], nl)\n",
    "        delta_new = deriv * wg\n",
    "        delta_prev = delta_new\n",
    "        gradients = [delta_new] + gradients\n",
    "    #Now compute the updates\n",
    "    #for g in gradients:\n",
    "        #print(\"gradient: \")\n",
    "        #print(g);\n",
    "    for i in range(0, out_idx + 1):\n",
    "        if i > 0:\n",
    "            upcurr = gradients[i].dot(np.transpose(A[i - 1]))\n",
    "        else:\n",
    "            upcurr = gradients[i].dot(X) \n",
    "       \n",
    "        upcurr = upcurr * rate\n",
    "        updates = updates + [upcurr]\n",
    "    return updates\n",
    "    \n",
    "up = backprop(X, activations, Y, WEIGHTS, 'sigmoid', 3.0)\n",
    "for i in range(0, len(up)):\n",
    "    print(\"update \" + str(i))\n",
    "    print(up[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 0\n",
      "[[ 0.71233037  0.71245816  0.71229407  0.7124211 ]\n",
      " [ 0.71228006  0.71243204  0.71224265  0.71239386]]\n",
      "it 1\n",
      "[[ 0.28886325  0.28866445  0.28891584  0.2887181 ]\n",
      " [ 0.28882811  0.28865254  0.28887943  0.28870491]]\n",
      "it 2\n",
      "[[ 0.71052454  0.71063078  0.71049328  0.71059888]\n",
      " [ 0.71048253  0.71061065  0.7104499   0.71057737]]\n",
      "it 3\n",
      "[[ 0.29041827  0.29024893  0.2904645   0.29029608]\n",
      " [ 0.29038881  0.29024057  0.29043357  0.29028622]]\n",
      "it 4\n",
      "[[ 0.70908838  0.70917847  0.70906098  0.70915053]\n",
      " [ 0.70905247  0.70916256  0.70902352  0.70913304]]\n",
      "it 5\n",
      "[[ 0.29167597  0.29152931  0.29171724  0.29157137]\n",
      " [ 0.29165073  0.29152341  0.29169035  0.2915638 ]]\n",
      "it 6\n",
      "[[ 0.70791857  0.70799618  0.70789422  0.70797137]\n",
      " [ 0.70788734  0.70798337  0.7078613   0.70795684]]\n",
      "it 7\n",
      "[[ 0.29271411  0.29258535  0.29275137  0.29262332]\n",
      " [ 0.29269209  0.29258121  0.29272759  0.29261738]]\n",
      "it 8\n",
      "[[ 0.70694714  0.70701488  0.70692527  0.7069926 ]\n",
      " [ 0.7069196   0.70700443  0.70689593  0.70698031]]\n",
      "it 9\n",
      "[[ 0.2935855   0.29347119  0.29361948  0.2935058 ]\n",
      " [ 0.29356604  0.29346834  0.29359815  0.29350105]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29affd9bfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "iterations = 10\n",
    "errors = []\n",
    "for i in range(0, iterations):\n",
    "    act = forward(X, WEIGHTS, 'sigmoid')\n",
    "    if i % 1 == 0:\n",
    "        print (\"it \" + str(i))\n",
    "        print(act[-1])\n",
    "    err = compute_error(act, np.transpose(Y))\n",
    "    #print (\"err at iter \" + str(i) + \": \" + str(err))\n",
    "    errors = errors + [err]\n",
    "    updates = backprop(X, act, Y, WEIGHTS, 'sigmoid', 10)\n",
    "    print \n",
    "    for j in range(0, len(WEIGHTS)):\n",
    "        WEIGHTS[j] = WEIGHTS[j] + updates[j]\n",
    "\n",
    "#print (\"errors: \" + str(errors))\n",
    "plt.plot(errors)\n",
    "\n",
    "plt.axis([0, len(errors), 0, 5])\n",
    "plt.ylabel('error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so this solves our toy example nicely. What about some real data? Let's try mnist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading...\n"
     ]
    }
   ],
   "source": [
    "print('loading...')\n",
    "images = np.loadtxt('c:\\\\users\\\\anthaue\\\\onedrive\\\\documents\\\\school\\\\mnist\\\\t10k-images-idx3-ubyte.mat')\n",
    "labels = np.loadtxt('c:\\\\users\\\\anthaue\\\\onedrive\\\\documents\\\\school\\\\mnist\\\\t10k-labels-idx1-ubyte.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEACAYAAAC3adEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFVNJREFUeJzt3X+UXWV97/H3N5MEoWTSEEu8ZiAU1HovQlFuICnc5QCl\nhpRrunpZEH6k1x+4srS0WVy1QW+zCFdstchqS2krWaaAvYFY0IuUZeWHOLIEIykSsTqBeKMhE34p\niSGEVQ2Tb/84J3H2MDM5Z+bsOXPOvF9rncXZ+zzzzHcPWfOZ/Tz72TsyE0mSDpjS7AIkSROLwSBJ\nKjAYJEkFBoMkqcBgkCQVGAySpIJSgyEi1kbE8xHxxAhtboiILRGxKSJOKbMeSdKhlX3GcDPwruE+\njIjzgBMy883AcuCzJdcjSTqEUoMhM78J7BqhyRLg89W23wZmRsScMmuSJI2s2XMMc4HtA7Z3VPdJ\nkpqk2cEgSZpgpjb5++8Ajhmw3VXd9xoR4U2dJGkUMjPqaT8eZwxRfQ3lbuAPACJiAfCzzHx+uI4y\ns21fV199ddNr8Pg8vsl2bJPh+Eaj1DOGiLgN6AZmR8TTwNXAdCAzc01mfiUiFkfED4G9wHvLrEeS\ndGilBkNmXlJDmyvKrEGSVB8nnyeI7u7uZpdQKo+vdbXzsUH7H99oxGjHoMZbRGSr1CpJE0VEkHVO\nPjf7qqQxO+6449i2bVuzy5jw5s2bx49//ONmlyGpBbT8GUM1DZtQUWvx5yRNTqM5Y3COQZJUYDBI\nkgoMBklSgcHQQvbv38+MGTPo6+trdimS2pjBUKIZM2bQ2dlJZ2cnHR0dHHHEEQf33X777XX3N2XK\nFPbs2UNXV1cJ1UpShVcljZPjjz+etWvXctZZZw3bpr+/n46OjlK+f6v8nCQ1llclTWBD3dBq1apV\nLF26lEsuuYSZM2eybt06NmzYwMKFC5k1axZz585lxYoV9Pf3A5XgmDJlCk8//TQAy5YtY8WKFSxe\nvJjOzk7OOOMM13RIGjODocnuuusuLrvsMnbv3s1FF13EtGnTuOGGG9i5cycPP/ww9957LzfddNPB\n9hHF4L/99tv55Cc/ya5duzjmmGNYtWrVeB+CpDYzKYIhYuyvspx55pksXrwYgMMOO4xTTz2V+fPn\nExEcd9xxfOADH+Ab3/jGwfaDzzouuOAC3v72t9PR0cGll17Kpk2byitW0qTQ8rfEqMVEHlo/5phj\nCttPPvkkH/7wh3nsscd45ZVX6O/v5/TTTx/269/whjccfH/EEUfw8ssvl1arpMlhUpwxTGSDh4aW\nL1/OSSedxNatW9m9ezfXXHONk8aSxpXBMMHs2bOHmTNncvjhh9Pb21uYX5Ck8WAwjJPBZwbDuf76\n67nlllvo7Ozkgx/8IEuXLh22n1r7lKR6uI5hkvDnJE1OrmOQJI2ZwSBJKjAYJEkFBoMkqcBgkCQV\nGAySpAKDQZJUYDBIkgoMBklSgcFQokY/2vOAhQsXcttttzWwUkn6pUlx2+1m2bNnz8H3tTzaU5Im\nAs8YxslQj/bcv38/n/jEJzjhhBM4+uijWbZsGS+99BIAr7zyChdffDGzZ89m1qxZLFy4kN27d/OR\nj3yEjRs3cvnll9PZ2clHP/rRZhyOpDZmMDTRddddxwMPPMAjjzxCX18f06ZN48orrwTgc5/7HP39\n/Tz77LO8+OKL3HjjjUyfPp3PfOYzzJ8/n7Vr1/LSSy9x3XXXNfkoJLWbSTGUFNeM/fbUeXXj70x6\n0003sW7dOubMmQPAqlWreNvb3sbatWuZNm0aP/nJT9iyZQsnnngip556arEe75QqqSSTIhjK+KXe\nCNu3b2fx4sUHn6tw4Jf9zp07ef/7389zzz3HBRdcwN69e1m2bBnXXnutz2CQVDqHkpqoq6uLBx98\nkJ07d7Jz50527drF3r17Oeqoo5g+fTrXXHMNvb29PPTQQ9xxxx2sX78e8AE9ksplMDTR8uXLWbly\nJX19fQC88MIL3HPPPQB87Wtfo7e3l8zkyCOPZOrUqXR0dAAwZ84ctm7d2rS6JbU3g2GcDPVX/sqV\nKzn33HM5++yzmTlzJmeeeSaPP/44ADt27GDJkiV0dnZy8sknc/7553PhhRcCcOWVV3Lrrbcye/Zs\nrrrqqnE9Dkntr/RHe0bEIuCvqITQ2sz89KDPO4H/CxwLdADXZ+YtQ/Tjoz3HwJ+TNDmN5tGepQZD\nREwBngLOAZ4BNgJLM3PzgDYfAzoz82MR8XrgSWBOZr46qC+DYQz8OUmT00R85vNpwJbM3JaZ+4D1\nwJJBbRKYUX0/A3hxcChIksZP2cEwF9g+YLuvum+gG4H/EhHPAN8FVpRckyRpBBNhHcO7gMcz8+yI\nOAG4PyJOzsyXBzdcvXr1wffd3d10d3ePW5GS1Ap6enro6ekZUx9lzzEsAFZn5qLq9lVADpyAjoh7\ngD/PzIer218DVmbmvw7qyzmGMfDnJE1OE3GOYSPwpoiYFxHTgaXA3YPabAN+GyAi5gBvAbxIX5Ka\npNShpMzsj4grgPv45eWqvRGxvPJxrgGuBW6JiCeqX/Ynmbmz1u8xb948VwLXYN68ec0uQVKLKH0d\nQ6MMN5QkSRreRBxKkiS1GINBklRgMEiSCgwGSVKBwSBJKjAYJEkFBoMkqcBgkCQVGAySpAKDQZJU\nYDBIkgoMBklSgcEgSSowGCRJBQaDJKnAYJAkFRgMkqQCg0GSVGAwSJIKDAZJUoHBIEkqMBgkSQUG\ngySpwGCQJBUYDJKkAoNBklRgMEiSCgwGSVKBwSBJKjAYJEkFIwZDRHRExGfGqxhJUvONGAyZ2Q+c\nOU61SJImgKk1tHk8Iu4G7gD2HtiZmV8qrSpJUtPUEgyvA14Ezh6wLwGDQZLaUGRms2uoSURkq9Qq\nSRNFRJCZUc/XHPKqpIjoioj/FxEvVF9fjIiuOopaFBGbI+KpiFg5TJvuiHg8Iv4tIr5ezwFIkhrr\nkGcMEXE/cBvwj9VdlwGXZua5h+w8YgrwFHAO8AywEViamZsHtJkJPAL8TmbuiIjXZ+ZPh+jLMwZJ\nqlMpZwzAr2XmzZn5avV1C/BrNfZ/GrAlM7dl5j5gPbBkUJtLgC9m5g6AoUJBkjR+agmGFyPisuqa\nho6IuIzKZHQt5gLbB2z3VfcN9BbgqIj4ekRsjIhlNfYtSSpBLVclvQ/4G+AvqVyN9Ajw3gbX8A4q\nVz39CvCtiPhWZv6wgd9DklSjEYMhIjqA38/Md4+y/x3AsQO2u6r7BuoDfpqZ/w78e0Q8BPwm8Jpg\nWL169cH33d3ddHd3j7IsSWpPPT099PT0jKmPWiafH83M00bVeSVYnqQy+fws8ChwcWb2DmjzVipn\nJIuAw4BvAxdl5g8G9eXksyTVaTSTz7UMJT0cETcCX6C48vk7h/rCzOyPiCuA+6jMZ6zNzN6IWF75\nONdk5uaIuBd4AugH1gwOBUnS+KnljGGodQWZmWcPsb80njFIUv1Gc8YwYjBU1yFckJn/NNbixspg\nkKT6NXwdQ2buB/5kTFVJklpKLUNJnwJ+ymvnGHaWW9pr6vCMQZLq1PChpGqnPxpid2bm8fV8o7Ey\nGCSpfqUEw0RhMEhS/cq6u+oREfGnEbGmuv3miDh/tEVKkia2Wu6VdDPwC+C3qts7gGtLq0iS1FS1\nBMMJmfkXwD6AzHwFqOu0RJLUOmoJhl9ExOFUbqBHRJwA/LzUqiRJTVPLLTGuBr4KHBMR64AzgPeU\nWZQkqXlquiopImYDC6gMIW1oxsN0vCpJkurn5aqSpIKyHu0pSZpEDAZJUsEhJ58j4qghdu/JzH0l\n1CNJarJazhi+A/wEeArYUn3/44j4TkScWmZxkqTxV0sw3A8szszXZ+Zs4DzgHuBDwN+VWZwkafzV\ncnfV72XmSYP2PZGZJ0fEpsw8pdQKf/k9vSpJkupU1jOfn42IlcD66vZFwPMR0QHsr7NGSdIEV8tQ\n0iVAF3BX9XVsdV8HcGF5pUmSmsEFbpLUxkoZSoqItwAfAY4b2D4zz663QEnSxFfL5PN3gc8CjwH9\nB/Zn5mPllvaaOjxjkKQ6lTX5/Gpm/v0oa5IktZhaJp//OSI+FBH/KSKOOvAqvTJJUlPUMpT0oyF2\nZ2YeX05Jw9bhUJIk1cnbbkuSCho6xxARZ2fmgxHx+0N9nplfqrdASdLEN9Lk8zuBB4H/PsRnCRgM\nktSGHEqSpDZW1gK3w4D/wWsXuP2feguUJE18taxj+DKwm8oCt5+XW44kqdlqCYauzFxUeiWSpAmh\nlgVuj0TESYduJklqB7UscPsB8CbgR1SGkoLKAreTyy+vUIeTz5JUp7LulXTeKOuRJLWgYYeSIqKz\n+nbPMK+aRMSiiNgcEU9VnwQ3XLv5EbFvuAV1kqTxMdIZw23A+VSuRkoqQ0gHJHDIeyVFxBTgRuAc\n4BlgY0R8OTM3D9HuU8C9dVUvSWq4YYMhM8+v/vfXx9D/acCWzNwGEBHrgSXA5kHt/gi4E5g/hu8l\nSWqAWuYYiIhZwJuB1x3Yl5kP1fClc4HtA7b7qITFwL7fCPxeZp4VEYXPJEnjr5aVz5cDK4AuYBOw\nAPgW0KhHe/4VMHDuoa7Zc0lSY9VyxrCCyhDPhupf9W8F/qzG/ncAxw7Y7qruG+i/AusjIoDXA+dF\nxL7MvHtwZ6tXrz74vru7m+7u7hrLkKTJoaenh56enjH1Ucs6ho2ZOT8iNgGnZ+bPI+L7mXniITuP\n6ACepDL5/CzwKHBxZvYO0/5m4J+HuqW36xgkqX5lrWPoi4hfBe4C7o+IXcC2WjrPzP6IuAK4j8ql\nsWszszcillc+zjWDv6SO2iVJJajrttsR8U5gJvDVzPxFaVUN/b09Y5CkOjX80Z7VoaDvZ+Zbx1rc\nWBkMklS/0QTDiDfRy8x+4MmIOHakdpKk9lHLHMMs4PsR8Siw98DOzHx3aVVJkpqmlmBYVXoVkqQJ\no5ZgWJyZhZvfRcSngW+UU5IkqZlqeVDPuUPs81bcktSmhj1jiIgPAh8Cjo+IJwZ8NAN4uOzCJEnN\nMezlqhExk8rE858DVw34aE9m7hyH2gbX4+WqklSnhq9jmEgMBkmqX8PXMUiSJh+DQZJUYDBIkgoM\nBklSgcEgSSowGCRJBQaDJKnAYJAkFRgMkqQCg0GSVGAwSJIKDAZJUoHBIEkqMBgkSQUGgySpwGCQ\nJBUYDJKkAoNBklRgMEiSCgwGSVKBwSBJKjAYJEkFBoMkqcBgkCQVGAySpAKDQZJUYDBIkgpKD4aI\nWBQRmyPiqYhYOcTnl0TEd6uvb0bESWXXJEkaXmRmeZ1HTAGeAs4BngE2Akszc/OANguA3szcHRGL\ngNWZuWCIvrLMWiWpHUUEmRn1fE3ZZwynAVsyc1tm7gPWA0sGNsjMDZm5u7q5AZhbck2SpBGUHQxz\nge0DtvsY+Rf/5cC/lFqRJGlEU5tdwAERcRbwXuDM4dqsXr364Pvu7m66u7tLr0uSWklPTw89PT1j\n6qPsOYYFVOYMFlW3rwIyMz89qN3JwBeBRZn5/4fpyzkGSarTRJxj2Ai8KSLmRcR0YClw98AGEXEs\nlVBYNlwoSJLGT6lDSZnZHxFXAPdRCaG1mdkbEcsrH+caYBVwFPB3ERHAvsw8rcy6JEnDK3UoqZEc\nSpKk+k3EoSRJUosxGCRJBQaDJKnAYJAkFRgMkqQCg0GSVGAwSJIKDAZJUoHBIEkqMBgkSQUGgySp\nwGCQJBUYDJKkAoNBklQwYR7tWYu4pq47x0rShJRXT+xHCLTU8xigNWqVpJGM56/d0TyPoaXOGFok\nwySppTnHIEkqMBgkSQUGgySpwGCQJBUYDJKkAoNBklRgMEiSCgwGSVKBwSBJKjAYJEkFBoMkqcBg\nkCQVGAySpAKDQZJUYDBIkgoMBklSgcEgSSowGCRJBaUHQ0QsiojNEfFURKwcps0NEbElIjZFxCll\n1yRJGl6pwRARU4AbgXcBJwIXR8RbB7U5DzghM98MLAc+W2ZNE1VPT0+zSyiVx9e62vnYoP2PbzTK\nPmM4DdiSmdsycx+wHlgyqM0S4PMAmfltYGZEzCm5rgmn3f9xenytq52PDdr/+Eaj7GCYC2wfsN1X\n3TdSmx1DtJEkjRMnnyVJBZGZ5XUesQBYnZmLqttXAZmZnx7Q5rPA1zPzC9XtzcA7M/P5QX2VV6gk\ntbHMjHraTy2rkKqNwJsiYh7wLLAUuHhQm7uBPwS+UA2Snw0OBaj/wCRJo1NqMGRmf0RcAdxHZdhq\nbWb2RsTyyse5JjO/EhGLI+KHwF7gvWXWJEkaWalDSZKk1tMSk8+1LJJrVRHRFREPRsT3I+J7EfHH\nza6p0SJiSkR8JyLubnYtjRYRMyPijojorf4/PL3ZNTVSRFwZEf8WEU9ExLqImN7smsYiItZGxPMR\n8cSAfbMi4r6IeDIi7o2Imc2scSyGOb6/qP773BQRX4yIzkP1M+GDoZZFci3uVeB/ZeaJwELgD9vs\n+ABWAD9odhEl+WvgK5n5n4HfBHqbXE/DRMQbgT8C3pGZJ1MZel7a3KrG7GYqv0sGugp4IDN/A3gQ\n+Ni4V9U4Qx3ffcCJmXkKsIUajm/CBwO1LZJrWZn5XGZuqr5/mcovlrZZxxERXcBi4HPNrqXRqn95\n/bfMvBkgM1/NzJeaXFajdQC/EhFTgSOAZ5pcz5hk5jeBXYN2LwFurb6/Ffi9cS2qgYY6vsx8IDP3\nVzc3AF2H6qcVgqGWRXJtISKOA04Bvt3cShrqL4GPAu04mfXrwE8j4ubqUNmaiDi82UU1SmY+A1wP\nPE1l4enPMvOB5lZViqMPXAmZmc8BRze5njK9D/iXQzVqhWCYFCLiSOBOYEX1zKHlRcTvAs9Xz4ii\n+monU4F3AH+bme8AXqEyLNEWIuJXqfw1PQ94I3BkRFzS3KrGRTv+EUNE/G9gX2bedqi2rRAMO4Bj\nB2x3Vfe1jepp+p3AP2bml5tdTwOdAbw7IrYCtwNnRcTnm1xTI/UB2zPzX6vbd1IJinbx28DWzNyZ\nmf3Al4DfanJNZXj+wP3ZIuINwAtNrqfhIuI9VIZ0awr2VgiGg4vkqldELKWyKK6d/APwg8z862YX\n0kiZ+fHMPDYzj6fy/+3BzPyDZtfVKNXhh+0R8ZbqrnNor0n2p4EFEfG6iAgqx9cOk+uDz17vBt5T\nff8/gVb/46xwfBGxiMpw7rsz8+e1dFD2yucxG26RXJPLapiIOAO4FPheRDxO5TT245n51eZWphr9\nMbAuIqYBW2mjBZqZ+WhE3Ak8Duyr/ndNc6sam4i4DegGZkfE08DVwKeAOyLifcA24MLmVTg2wxzf\nx4HpwP2VfGdDZn5oxH5c4CZJGqgVhpIkSePIYJAkFRgMkqQCg0GSVGAwSJIKDAZJUoHBIEkqMBgk\nSQX/AbBR6t76EwvkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29a80353048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 70000.0\n",
      "testacc: 0.11399999999999999\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-b7ef60a1a35a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWEIGHTS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnonlinfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[0merr\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcompute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[0mupdates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWEIGHTS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnonlinfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWEIGHTS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[0mWEIGHTS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWEIGHTS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mupdates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-46008cd27215>\u001b[0m in \u001b[0;36mbackprop\u001b[1;34m(X, A, Y, W, nl, rate)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_idx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0mupcurr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mupcurr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEACAYAAAC3adEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFVNJREFUeJzt3X+UXWV97/H3N5MEoWTSEEu8ZiAU1HovQlFuICnc5QCl\nhpRrunpZEH6k1x+4srS0WVy1QW+zCFdstchqS2krWaaAvYFY0IuUZeWHOLIEIykSsTqBeKMhE34p\niSGEVQ2Tb/84J3H2MDM5Z+bsOXPOvF9rncXZ+zzzzHcPWfOZ/Tz72TsyE0mSDpjS7AIkSROLwSBJ\nKjAYJEkFBoMkqcBgkCQVGAySpIJSgyEi1kbE8xHxxAhtboiILRGxKSJOKbMeSdKhlX3GcDPwruE+\njIjzgBMy883AcuCzJdcjSTqEUoMhM78J7BqhyRLg89W23wZmRsScMmuSJI2s2XMMc4HtA7Z3VPdJ\nkpqk2cEgSZpgpjb5++8Ajhmw3VXd9xoR4U2dJGkUMjPqaT8eZwxRfQ3lbuAPACJiAfCzzHx+uI4y\ns21fV199ddNr8Pg8vsl2bJPh+Eaj1DOGiLgN6AZmR8TTwNXAdCAzc01mfiUiFkfED4G9wHvLrEeS\ndGilBkNmXlJDmyvKrEGSVB8nnyeI7u7uZpdQKo+vdbXzsUH7H99oxGjHoMZbRGSr1CpJE0VEkHVO\nPjf7qqQxO+6449i2bVuzy5jw5s2bx49//ONmlyGpBbT8GUM1DZtQUWvx5yRNTqM5Y3COQZJUYDBI\nkgoMBklSgcHQQvbv38+MGTPo6+trdimS2pjBUKIZM2bQ2dlJZ2cnHR0dHHHEEQf33X777XX3N2XK\nFPbs2UNXV1cJ1UpShVcljZPjjz+etWvXctZZZw3bpr+/n46OjlK+f6v8nCQ1llclTWBD3dBq1apV\nLF26lEsuuYSZM2eybt06NmzYwMKFC5k1axZz585lxYoV9Pf3A5XgmDJlCk8//TQAy5YtY8WKFSxe\nvJjOzk7OOOMM13RIGjODocnuuusuLrvsMnbv3s1FF13EtGnTuOGGG9i5cycPP/ww9957LzfddNPB\n9hHF4L/99tv55Cc/ya5duzjmmGNYtWrVeB+CpDYzKYIhYuyvspx55pksXrwYgMMOO4xTTz2V+fPn\nExEcd9xxfOADH+Ab3/jGwfaDzzouuOAC3v72t9PR0cGll17Kpk2byitW0qTQ8rfEqMVEHlo/5phj\nCttPPvkkH/7wh3nsscd45ZVX6O/v5/TTTx/269/whjccfH/EEUfw8ssvl1arpMlhUpwxTGSDh4aW\nL1/OSSedxNatW9m9ezfXXHONk8aSxpXBMMHs2bOHmTNncvjhh9Pb21uYX5Ck8WAwjJPBZwbDuf76\n67nlllvo7Ozkgx/8IEuXLh22n1r7lKR6uI5hkvDnJE1OrmOQJI2ZwSBJKjAYJEkFBoMkqcBgkCQV\nGAySpAKDQZJUYDBIkgoMBklSgcFQokY/2vOAhQsXcttttzWwUkn6pUlx2+1m2bNnz8H3tTzaU5Im\nAs8YxslQj/bcv38/n/jEJzjhhBM4+uijWbZsGS+99BIAr7zyChdffDGzZ89m1qxZLFy4kN27d/OR\nj3yEjRs3cvnll9PZ2clHP/rRZhyOpDZmMDTRddddxwMPPMAjjzxCX18f06ZN48orrwTgc5/7HP39\n/Tz77LO8+OKL3HjjjUyfPp3PfOYzzJ8/n7Vr1/LSSy9x3XXXNfkoJLWbSTGUFNeM/fbUeXXj70x6\n0003sW7dOubMmQPAqlWreNvb3sbatWuZNm0aP/nJT9iyZQsnnngip556arEe75QqqSSTIhjK+KXe\nCNu3b2fx4sUHn6tw4Jf9zp07ef/7389zzz3HBRdcwN69e1m2bBnXXnutz2CQVDqHkpqoq6uLBx98\nkJ07d7Jz50527drF3r17Oeqoo5g+fTrXXHMNvb29PPTQQ9xxxx2sX78e8AE9ksplMDTR8uXLWbly\nJX19fQC88MIL3HPPPQB87Wtfo7e3l8zkyCOPZOrUqXR0dAAwZ84ctm7d2rS6JbU3g2GcDPVX/sqV\nKzn33HM5++yzmTlzJmeeeSaPP/44ADt27GDJkiV0dnZy8sknc/7553PhhRcCcOWVV3Lrrbcye/Zs\nrrrqqnE9Dkntr/RHe0bEIuCvqITQ2sz89KDPO4H/CxwLdADXZ+YtQ/Tjoz3HwJ+TNDmN5tGepQZD\nREwBngLOAZ4BNgJLM3PzgDYfAzoz82MR8XrgSWBOZr46qC+DYQz8OUmT00R85vNpwJbM3JaZ+4D1\nwJJBbRKYUX0/A3hxcChIksZP2cEwF9g+YLuvum+gG4H/EhHPAN8FVpRckyRpBBNhHcO7gMcz8+yI\nOAG4PyJOzsyXBzdcvXr1wffd3d10d3ePW5GS1Ap6enro6ekZUx9lzzEsAFZn5qLq9lVADpyAjoh7\ngD/PzIer218DVmbmvw7qyzmGMfDnJE1OE3GOYSPwpoiYFxHTgaXA3YPabAN+GyAi5gBvAbxIX5Ka\npNShpMzsj4grgPv45eWqvRGxvPJxrgGuBW6JiCeqX/Ynmbmz1u8xb948VwLXYN68ec0uQVKLKH0d\nQ6MMN5QkSRreRBxKkiS1GINBklRgMEiSCgwGSVKBwSBJKjAYJEkFBoMkqcBgkCQVGAySpAKDQZJU\nYDBIkgoMBklSgcEgSSowGCRJBQaDJKnAYJAkFRgMkqQCg0GSVGAwSJIKDAZJUoHBIEkqMBgkSQUG\ngySpwGCQJBUYDJKkAoNBklRgMEiSCgwGSVKBwSBJKjAYJEkFIwZDRHRExGfGqxhJUvONGAyZ2Q+c\nOU61SJImgKk1tHk8Iu4G7gD2HtiZmV8qrSpJUtPUEgyvA14Ezh6wLwGDQZLaUGRms2uoSURkq9Qq\nSRNFRJCZUc/XHPKqpIjoioj/FxEvVF9fjIiuOopaFBGbI+KpiFg5TJvuiHg8Iv4tIr5ezwFIkhrr\nkGcMEXE/cBvwj9VdlwGXZua5h+w8YgrwFHAO8AywEViamZsHtJkJPAL8TmbuiIjXZ+ZPh+jLMwZJ\nqlMpZwzAr2XmzZn5avV1C/BrNfZ/GrAlM7dl5j5gPbBkUJtLgC9m5g6AoUJBkjR+agmGFyPisuqa\nho6IuIzKZHQt5gLbB2z3VfcN9BbgqIj4ekRsjIhlNfYtSSpBLVclvQ/4G+AvqVyN9Ajw3gbX8A4q\nVz39CvCtiPhWZv6wgd9DklSjEYMhIjqA38/Md4+y/x3AsQO2u6r7BuoDfpqZ/w78e0Q8BPwm8Jpg\nWL169cH33d3ddHd3j7IsSWpPPT099PT0jKmPWiafH83M00bVeSVYnqQy+fws8ChwcWb2DmjzVipn\nJIuAw4BvAxdl5g8G9eXksyTVaTSTz7UMJT0cETcCX6C48vk7h/rCzOyPiCuA+6jMZ6zNzN6IWF75\nONdk5uaIuBd4AugH1gwOBUnS+KnljGGodQWZmWcPsb80njFIUv1Gc8YwYjBU1yFckJn/NNbixspg\nkKT6NXwdQ2buB/5kTFVJklpKLUNJnwJ+ymvnGHaWW9pr6vCMQZLq1PChpGqnPxpid2bm8fV8o7Ey\nGCSpfqUEw0RhMEhS/cq6u+oREfGnEbGmuv3miDh/tEVKkia2Wu6VdDPwC+C3qts7gGtLq0iS1FS1\nBMMJmfkXwD6AzHwFqOu0RJLUOmoJhl9ExOFUbqBHRJwA/LzUqiRJTVPLLTGuBr4KHBMR64AzgPeU\nWZQkqXlquiopImYDC6gMIW1oxsN0vCpJkurn5aqSpIKyHu0pSZpEDAZJUsEhJ58j4qghdu/JzH0l\n1CNJarJazhi+A/wEeArYUn3/44j4TkScWmZxkqTxV0sw3A8szszXZ+Zs4DzgHuBDwN+VWZwkafzV\ncnfV72XmSYP2PZGZJ0fEpsw8pdQKf/k9vSpJkupU1jOfn42IlcD66vZFwPMR0QHsr7NGSdIEV8tQ\n0iVAF3BX9XVsdV8HcGF5pUmSmsEFbpLUxkoZSoqItwAfAY4b2D4zz663QEnSxFfL5PN3gc8CjwH9\nB/Zn5mPllvaaOjxjkKQ6lTX5/Gpm/v0oa5IktZhaJp//OSI+FBH/KSKOOvAqvTJJUlPUMpT0oyF2\nZ2YeX05Jw9bhUJIk1cnbbkuSCho6xxARZ2fmgxHx+0N9nplfqrdASdLEN9Lk8zuBB4H/PsRnCRgM\nktSGHEqSpDZW1gK3w4D/wWsXuP2feguUJE18taxj+DKwm8oCt5+XW44kqdlqCYauzFxUeiWSpAmh\nlgVuj0TESYduJklqB7UscPsB8CbgR1SGkoLKAreTyy+vUIeTz5JUp7LulXTeKOuRJLWgYYeSIqKz\n+nbPMK+aRMSiiNgcEU9VnwQ3XLv5EbFvuAV1kqTxMdIZw23A+VSuRkoqQ0gHJHDIeyVFxBTgRuAc\n4BlgY0R8OTM3D9HuU8C9dVUvSWq4YYMhM8+v/vfXx9D/acCWzNwGEBHrgSXA5kHt/gi4E5g/hu8l\nSWqAWuYYiIhZwJuB1x3Yl5kP1fClc4HtA7b7qITFwL7fCPxeZp4VEYXPJEnjr5aVz5cDK4AuYBOw\nAPgW0KhHe/4VMHDuoa7Zc0lSY9VyxrCCyhDPhupf9W8F/qzG/ncAxw7Y7qruG+i/AusjIoDXA+dF\nxL7MvHtwZ6tXrz74vru7m+7u7hrLkKTJoaenh56enjH1Ucs6ho2ZOT8iNgGnZ+bPI+L7mXniITuP\n6ACepDL5/CzwKHBxZvYO0/5m4J+HuqW36xgkqX5lrWPoi4hfBe4C7o+IXcC2WjrPzP6IuAK4j8ql\nsWszszcillc+zjWDv6SO2iVJJajrttsR8U5gJvDVzPxFaVUN/b09Y5CkOjX80Z7VoaDvZ+Zbx1rc\nWBkMklS/0QTDiDfRy8x+4MmIOHakdpKk9lHLHMMs4PsR8Siw98DOzHx3aVVJkpqmlmBYVXoVkqQJ\no5ZgWJyZhZvfRcSngW+UU5IkqZlqeVDPuUPs81bcktSmhj1jiIgPAh8Cjo+IJwZ8NAN4uOzCJEnN\nMezlqhExk8rE858DVw34aE9m7hyH2gbX4+WqklSnhq9jmEgMBkmqX8PXMUiSJh+DQZJUYDBIkgoM\nBklSgcEgSSowGCRJBQaDJKnAYJAkFRgMkqQCg0GSVGAwSJIKDAZJUoHBIEkqMBgkSQUGgySpwGCQ\nJBUYDJKkAoNBklRgMEiSCgwGSVKBwSBJKjAYJEkFBoMkqcBgkCQVGAySpAKDQZJUYDBIkgpKD4aI\nWBQRmyPiqYhYOcTnl0TEd6uvb0bESWXXJEkaXmRmeZ1HTAGeAs4BngE2Akszc/OANguA3szcHRGL\ngNWZuWCIvrLMWiWpHUUEmRn1fE3ZZwynAVsyc1tm7gPWA0sGNsjMDZm5u7q5AZhbck2SpBGUHQxz\nge0DtvsY+Rf/5cC/lFqRJGlEU5tdwAERcRbwXuDM4dqsXr364Pvu7m66u7tLr0uSWklPTw89PT1j\n6qPsOYYFVOYMFlW3rwIyMz89qN3JwBeBRZn5/4fpyzkGSarTRJxj2Ai8KSLmRcR0YClw98AGEXEs\nlVBYNlwoSJLGT6lDSZnZHxFXAPdRCaG1mdkbEcsrH+caYBVwFPB3ERHAvsw8rcy6JEnDK3UoqZEc\nSpKk+k3EoSRJUosxGCRJBQaDJKnAYJAkFRgMkqQCg0GSVGAwSJIKDAZJUoHBIEkqMBgkSQUGgySp\nwGCQJBUYDJKkAoNBklQwYR7tWYu4pq47x0rShJRXT+xHCLTU8xigNWqVpJGM56/d0TyPoaXOGFok\nwySppTnHIEkqMBgkSQUGgySpwGCQJBUYDJKkAoNBklRgMEiSCgwGSVKBwSBJKjAYJEkFBoMkqcBg\nkCQVGAySpAKDQZJUYDBIkgoMBklSgcEgSSowGCRJBaUHQ0QsiojNEfFURKwcps0NEbElIjZFxCll\n1yRJGl6pwRARU4AbgXcBJwIXR8RbB7U5DzghM98MLAc+W2ZNE1VPT0+zSyiVx9e62vnYoP2PbzTK\nPmM4DdiSmdsycx+wHlgyqM0S4PMAmfltYGZEzCm5rgmn3f9xenytq52PDdr/+Eaj7GCYC2wfsN1X\n3TdSmx1DtJEkjRMnnyVJBZGZ5XUesQBYnZmLqttXAZmZnx7Q5rPA1zPzC9XtzcA7M/P5QX2VV6gk\ntbHMjHraTy2rkKqNwJsiYh7wLLAUuHhQm7uBPwS+UA2Snw0OBaj/wCRJo1NqMGRmf0RcAdxHZdhq\nbWb2RsTyyse5JjO/EhGLI+KHwF7gvWXWJEkaWalDSZKk1tMSk8+1LJJrVRHRFREPRsT3I+J7EfHH\nza6p0SJiSkR8JyLubnYtjRYRMyPijojorf4/PL3ZNTVSRFwZEf8WEU9ExLqImN7smsYiItZGxPMR\n8cSAfbMi4r6IeDIi7o2Imc2scSyGOb6/qP773BQRX4yIzkP1M+GDoZZFci3uVeB/ZeaJwELgD9vs\n+ABWAD9odhEl+WvgK5n5n4HfBHqbXE/DRMQbgT8C3pGZJ1MZel7a3KrG7GYqv0sGugp4IDN/A3gQ\n+Ni4V9U4Qx3ffcCJmXkKsIUajm/CBwO1LZJrWZn5XGZuqr5/mcovlrZZxxERXcBi4HPNrqXRqn95\n/bfMvBkgM1/NzJeaXFajdQC/EhFTgSOAZ5pcz5hk5jeBXYN2LwFurb6/Ffi9cS2qgYY6vsx8IDP3\nVzc3AF2H6qcVgqGWRXJtISKOA04Bvt3cShrqL4GPAu04mfXrwE8j4ubqUNmaiDi82UU1SmY+A1wP\nPE1l4enPMvOB5lZViqMPXAmZmc8BRze5njK9D/iXQzVqhWCYFCLiSOBOYEX1zKHlRcTvAs9Xz4ii\n+monU4F3AH+bme8AXqEyLNEWIuJXqfw1PQ94I3BkRFzS3KrGRTv+EUNE/G9gX2bedqi2rRAMO4Bj\nB2x3Vfe1jepp+p3AP2bml5tdTwOdAbw7IrYCtwNnRcTnm1xTI/UB2zPzX6vbd1IJinbx28DWzNyZ\nmf3Al4DfanJNZXj+wP3ZIuINwAtNrqfhIuI9VIZ0awr2VgiGg4vkqldELKWyKK6d/APwg8z862YX\n0kiZ+fHMPDYzj6fy/+3BzPyDZtfVKNXhh+0R8ZbqrnNor0n2p4EFEfG6iAgqx9cOk+uDz17vBt5T\nff8/gVb/46xwfBGxiMpw7rsz8+e1dFD2yucxG26RXJPLapiIOAO4FPheRDxO5TT245n51eZWphr9\nMbAuIqYBW2mjBZqZ+WhE3Ak8Duyr/ndNc6sam4i4DegGZkfE08DVwKeAOyLifcA24MLmVTg2wxzf\nx4HpwP2VfGdDZn5oxH5c4CZJGqgVhpIkSePIYJAkFRgMkqQCg0GSVGAwSJIKDAZJUoHBIEkqMBgk\nSQX/AbBR6t76EwvkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29a80353048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The mnist labels are a single dimension, but we'd rather\n",
    "#have them be a 10-dimensional array with 1 in the target dim\n",
    "#and zeros elsewhere else.\n",
    "import math\n",
    "def vectorize_targets(x):\n",
    "    ret = np.ones(([len(x), np.max(x) + 1]))\n",
    "    #Try setting to -1 instead of zero for tanh learner.\n",
    "    ret = ret * -1\n",
    "    print (ret)\n",
    "    for i in range(0, len(x)):\n",
    "        ret[i][x[i]] = 1\n",
    "    return ret\n",
    "#Need the bias ones on the training examples\n",
    "def prepend_ones_row(x):\n",
    "    return np.hstack((np.ones([len(x), 1]), x))\n",
    "    \n",
    "#We assume symmetric around zero.\n",
    "def init_random_matrix(rows, cols, magnitude):\n",
    "    ret = np.random.rand(rows, cols)\n",
    "    ret = ret - 0.5\n",
    "    ret = ret * (2 * magnitude)\n",
    "    return ret\n",
    "    \n",
    "def init_weights(X, Y, hiddenlen):\n",
    "    #magnitude = 0.05\n",
    "    (xrows, xcols) = X.shape\n",
    "    (yrows, ycols) = Y.shape\n",
    "    mag1 = 1 / math.sqrt(xcols)\n",
    "    x2hidden = init_random_matrix(hiddenlen, xcols, mag1)\n",
    "    mag2 = 1 / math.sqrt(hiddenlen)\n",
    "    hidden1hidden2 = init_random_matrix(hiddenlen, hiddenlen, mag2)\n",
    "    #hidden2hidden3 = init_random_matrix(hiddenlen, hiddenlen, magnitude)\n",
    "    #hidden3hidden4 = init_random_matrix(hiddenlen, hiddenlen, magnitude)\n",
    "    hidden2out = init_random_matrix(ycols, hiddenlen, mag2)\n",
    "    #return[x2hidden, hidden1hidden2, hidden2hidden3, hidden3hidden4, hidden4out]\n",
    "    return[x2hidden, hidden1hidden2, hidden2out]\n",
    "\n",
    "def normalize_inputs(normalizeMe, minscaled, maxscaled):\n",
    "    minunscaled = np.min(normalizeMe)\n",
    "    maxunscaled = np.max(normalizeMe)\n",
    "    ratio = (maxunscaled - minunscaled) / (maxscaled - minscaled)\n",
    "    print('minunscaled ' + str(minunscaled) + ', maxunscaled ' + str(maxunscaled) + ', ratio ' + str(ratio))\n",
    "    return minscaled + ((normalizeMe - minunscaled) / ratio)\n",
    "\n",
    "def split(samples, labels, trainpercent):\n",
    "    (rows, cols) = samples.shape\n",
    "    allRows = np.arange(rows)\n",
    "    np.random.shuffle(allRows)\n",
    "    cutoffRow = rows * trainpercent\n",
    "    trainIndices=allRows[0:cutoffRow]\n",
    "    testIndices=allRows[cutoffRow:]\n",
    "    trainSamples = samples[trainIndices]\n",
    "    trainLabels = labels[trainIndices]\n",
    "    testSamples = samples[testIndices]\n",
    "    testLabels = labels[testIndices]\n",
    "    return(trainSamples, trainLabels, testSamples, testLabels)\n",
    "    \n",
    "def accuracy(output, labels):\n",
    "    predmax = np.argmax(output, 1)\n",
    "    labmax = np.argmax(labels, 1)\n",
    "    diff = predmax - labmax\n",
    "    errors = np.count_nonzero(diff)\n",
    "    return 1.0 - (errors / len(output))\n",
    "    \n",
    "\n",
    "print('vectorizing...')\n",
    "veclabels = vectorize_targets(labels)\n",
    "print(veclabels)\n",
    "print('normalizing')\n",
    "normalized = normalize_inputs(images, -1, 1)\n",
    "print('biasing...')\n",
    "biased = prepend_ones_row(normalized)\n",
    "print(biased)\n",
    "\n",
    "(trainSamples, trainLabels, testSamples, testLabels) = split(biased, veclabels, 0.7)\n",
    "\n",
    "print('initializing weights...')\n",
    "WEIGHTS = init_weights(trainSamples, trainLabels, 200)\n",
    "\n",
    "plt.plot([], [])\n",
    "plt.show()\n",
    "from IPython import display\n",
    "\n",
    "iterations = 1000\n",
    "errors = []\n",
    "trainAccs = []\n",
    "testAccs = []\n",
    "mb_size = 10\n",
    "nonlinfunc = 'sigmoid'\n",
    "for i in range(0, iterations):\n",
    "    err=0\n",
    "    #Shuffle training cases.\n",
    "    order = np.arange(0, len(trainSamples))\n",
    "    np.random.shuffle(order)\n",
    "    trainSamples = trainSamples[order]\n",
    "    trainLabels = trainLabels[order]\n",
    "    for mbstart in range(0, len(trainSamples), mb_size):\n",
    "        mbend = min(len(trainSamples), mbstart + mb_size)\n",
    "        currX = trainSamples[mbstart:mbend,:]\n",
    "        currY = trainLabels[mbstart:mbend,:]\n",
    "        act = forward(currX, WEIGHTS, nonlinfunc)\n",
    "        err += compute_error(act, np.transpose(currY))\n",
    "        updates = backprop(currX, act, currY, WEIGHTS, nonlinfunc, 5.0)\n",
    "        for j in range(0, len(WEIGHTS)):\n",
    "            WEIGHTS[j] = WEIGHTS[j] + updates[j]\n",
    "    errors = errors + [err]\n",
    "    \n",
    "    trainOut = forward(trainSamples, WEIGHTS, nonlinfunc)\n",
    "    testOut = forward(testSamples, WEIGHTS, nonlinfunc)\n",
    "    trainAcc = accuracy(trainOut[-1].transpose(), trainLabels)\n",
    "    testAcc = accuracy(testOut[-1].transpose(), testLabels)\n",
    "    trainAccs = trainAccs + [trainAcc]\n",
    "    testAccs = testAccs + [testAcc]\n",
    "    \n",
    "\n",
    "#plt.plot(errors)\n",
    "    rng = np.arange(len(trainAccs))\n",
    "    #hl.set_xdata(rng)\n",
    "    #hl.set_ydata(trainAccs)\n",
    "    #hl.draw();\n",
    "    plt.clf()\n",
    "    plt.plot(rng, trainAccs, label='Train')\n",
    "    plt.plot(rng, testAccs, label='Test')\n",
    "    plt.legend(loc=2)\n",
    "    plt.axis([0, len(trainAccs), 0, 1.0])\n",
    "    plt.ylabel('training error')\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    #plt.show()\n",
    "    print('error: ' + str(err))\n",
    "    print('testacc: ' + str(testAcc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
